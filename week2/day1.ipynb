{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with the APIs for Anthropic and Google, as well as OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Important Note - Please read me</h2>\n",
    "            <span style=\"color:#900;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull and merge your changes as needed</a>. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/><br/>\n",
    "            After you've pulled the code, from the llm_engineering directory, in an Anaconda prompt (PC) or Terminal (Mac), run:<br/>\n",
    "            <code>conda env update --f environment.yml</code><br/>\n",
    "            Or if you used virtualenv rather than Anaconda, then run this from your activated environment in a Powershell (PC) or Terminal (Mac):<br/>\n",
    "            <code>pip install -r requirements.txt</code>\n",
    "            <br/>Then restart the kernel (Kernel menu >> Restart Kernel and Clear Outputs Of All Cells) to pick up the changes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder about the resources page</h2>\n",
    "            <span style=\"color:#f71;\">Here's a link to resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you could now create API keys for Anthropic and Google in addition to OpenAI.\n",
    "\n",
    "**Please note:** if you'd prefer to avoid extra API costs, feel free to skip setting up Anthopic and Google! You can see me do it, and focus on OpenAI for the course. You could also substitute Anthropic and/or Google for Ollama, using the exercise you did in week 1.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "### Also - adding DeepSeek if you wish\n",
    "\n",
    "Optionally, if you'd like to also use DeepSeek, create an account [here](https://platform.deepseek.com/), create a key [here](https://platform.deepseek.com/api_keys) and top up with at least the minimum $2 [here](https://platform.deepseek.com/top_up).\n",
    "\n",
    "### Adding API keys to your .env file\n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "Afterwards, you may need to restart the Jupyter Lab Kernel (the Python process that sits behind this notebook) via the Kernel menu, and then rerun the cells from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for google\n",
    "# in rare cases, this seems to give an error on some systems, or even crashes the kernel\n",
    "# If this happens to you, simply ignore this cell - I give an alternative approach for using Gemini later\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyCa\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "425ed580-808d-429b-85b0-6cba50ca1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the set up code for Gemini\n",
    "# Having problems with Google Gemini setup? Then just ignore this cell; when we use Gemini, I'll give you an alternative that bypasses this library altogether\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to work?\n",
      "\n",
      "Because they heard the job was about scaling up!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-4o-mini', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician?\n",
      "\n",
      "Because they couldn't find a significant connection!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12d2a549-9d6e-4ea0-9c3e-b96a39e9959e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the histogram?\n",
      "\n",
      "Because it just wasn’t her type!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1-nano - extremely fast and cheap\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1-nano',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the logistic regression model?\n",
      "\n",
      "Because it just couldn't commit!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1',\n",
    "    messages=prompts,\n",
    "    temperature=0.8\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96232ef4-dc9e-430b-a9df-f516685e7c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to work? \n",
      "\n",
      "Because she heard the model was growing and needed help scaling!\n"
     ]
    }
   ],
   "source": [
    "# If you have access to this, here is the reasoning model o3-mini\n",
    "# This is trained to think through its response before replying\n",
    "# So it will take longer but the answer should be more reasoned - not that this helps..\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='o3-mini',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't data scientists like playing hide and seek?\n",
      "\n",
      "Because they always find the pattern in where you're hiding!\n"
     ]
    }
   ],
   "source": [
    "# Claude 3.7 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-7-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't data scientists like to go to the beach?\n",
      "\n",
      "Because they're afraid of getting caught in an infinite loop of waves!\n",
      "\n",
      "*Ba-dum-tss* 🥁"
     ]
    }
   ],
   "source": [
    "# Claude 3.7 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "# If the streaming looks strange, then please see the note below this cell!\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-7-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.4,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e17bc-cd46-4c23-b639-0c7b748e6c5a",
   "metadata": {},
   "source": [
    "## A rare problem with Claude streaming on some Windows boxes\n",
    "\n",
    "2 students have noticed a strange thing happening with Claude's streaming into Jupyter Lab's output -- it sometimes seems to swallow up parts of the response.\n",
    "\n",
    "To fix this, replace the code:\n",
    "\n",
    "`print(text, end=\"\", flush=True)`\n",
    "\n",
    "with this:\n",
    "\n",
    "`clean_text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")`  \n",
    "`print(clean_text, end=\"\", flush=True)`\n",
    "\n",
    "And it should work fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the Python data scientist break up with the R data scientist?\n",
      "\n",
      "Because they couldn't see eye to eye on the right way to handle missing values! She thought imputation was the key, but he was all about the `na.omit` life! It was a real `NaN`-sense argument!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The API for Gemini has a slightly different structure.\n",
    "# I've heard that on some PCs, this Gemini code causes the Kernel to crash.\n",
    "# If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-2.0-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49009a30-037d-41c8-b874-127f61c4aa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As an alternative way to use Gemini that bypasses Google's python API library,\n",
    "# Google released endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
    "# We're also trying Gemini's latest reasoning/thinking model\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-2.5-flash-preview-04-17\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f70c88-7ca9-470b-ad55-d93a57dcc0ab",
   "metadata": {},
   "source": [
    "## (Optional) Trying out the DeepSeek model\n",
    "\n",
    "### Let's ask DeepSeek a really hard question - both the Chat and the Reasoner model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0019fb-f6a8-45cb-962b-ef8bf7070d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally if you wish to try DeekSeek, you can also use the OpenAI client library\n",
    "\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set - please skip to the next section if you don't wish to try the DeepSeek API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c871e-68d6-4668-9c27-96d52b77b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Chat\n",
    "\n",
    "deepseek_via_openai_client = OpenAI(\n",
    "    api_key=deepseek_api_key, \n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=prompts,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b6e70f-700a-46cf-942f-659101ffeceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = [{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "             {\"role\": \"user\", \"content\": \"How many words are there in your answer to this prompt\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d1151c-2015-4e37-80c8-16bc16367cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Chat with a harder question! And streaming results\n",
    "\n",
    "stream = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=challenge,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)\n",
    "\n",
    "print(\"Number of words:\", len(reply.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a93f7d-9300-48cc-8c1a-ee67380db495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Reasoner - this may hit an error if DeepSeek is busy\n",
    "# It's over-subscribed (as of 28-Jan-2025) but should come back online soon!\n",
    "# If this fails, come back to this in a few days..\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    messages=challenge\n",
    ")\n",
    "\n",
    "reasoning_content = response.choices[0].message.reasoning_content\n",
    "content = response.choices[0].message.content\n",
    "\n",
    "print(reasoning_content)\n",
    "print(content)\n",
    "print(\"Number of words:\", len(content.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf0d5dd-7f20-4090-a46d-da56ceec218f",
   "metadata": {},
   "source": [
    "## Additional exercise to build your experience with the models\n",
    "\n",
    "This is optional, but if you have time, it's so great to get first hand experience with the capabilities of these different models.\n",
    "\n",
    "You could go back and ask the same question via the APIs above to get your own personal experience with the pros & cons of the models.\n",
    "\n",
    "Later in the course we'll look at benchmarks and compare LLMs on many dimensions. But nothing beats personal experience!\n",
    "\n",
    "Here are some questions to try:\n",
    "1. The question above: \"How many words are there in your answer to this prompt\"\n",
    "2. A creative question: \"In 3 sentences, describe the color Blue to someone who's never been able to see\"\n",
    "3. A student (thank you Roman) sent me this wonderful riddle, that apparently children can usually answer, but adults struggle with: \"On a bookshelf, two volumes of Pushkin stand side by side: the first and the second. The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick. A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume. What distance did it gnaw through?\".\n",
    "\n",
    "The answer may not be what you expect, and even though I'm quite good at puzzles, I'm embarrassed to admit that I got this one wrong.\n",
    "\n",
    "### What to look out for as you experiment with models\n",
    "\n",
    "1. How the Chat models differ from the Reasoning models (also known as Thinking models)\n",
    "2. The ability to solve problems and the ability to be creative\n",
    "3. Speed of generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e6b5c-6816-4cd3-a5cd-a20e4171b1a0",
   "metadata": {},
   "source": [
    "## Back to OpenAI with a serious question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Deciding if a Business Problem is Suitable for an LLM Solution\n",
       "\n",
       "When evaluating whether a business problem is suitable for a Large Language Model (LLM) solution, consider the following criteria:\n",
       "\n",
       "## 1. Nature of the Task\n",
       "- **Text-Based**: LLMs excel at tasks involving natural language processing, such as:\n",
       "  - Text generation\n",
       "  - Summarization\n",
       "  - Translation\n",
       "  - Sentiment analysis\n",
       "  - Question answering\n",
       "\n",
       "- **Complexity**: If the problem requires understanding context, nuances, or generating human-like responses, LLMs may be appropriate.\n",
       "\n",
       "## 2. Data Availability\n",
       "- **Text Data**: Ensure you have access to high-quality text data related to the problem.\n",
       "- **Volume**: LLMs often benefit from large datasets for fine-tuning or training.\n",
       "- **Quality**: The data should be relevant and representative of the problem domain.\n",
       "\n",
       "## 3. User Interaction\n",
       "- **Natural Language Input**: If the solution involves user interactions through chat, emails, or voice, LLMs can provide a more intuitive experience.\n",
       "- **Dynamic Responses**: For applications requiring dynamic conversation or content generation, LLMs can adapt to user input effectively.\n",
       "\n",
       "## 4. Business Goals\n",
       "- **Efficiency**: Evaluate if an LLM can automate or streamline processes, reducing manual effort.\n",
       "- **Value Addition**: Assess whether an LLM can provide insights or enhance decision-making.\n",
       "- **User Experience**: Consider if an LLM can elevate customer engagement or satisfaction.\n",
       "\n",
       "## 5. Feasibility and Resources\n",
       "- **Technical Expertise**: Ensure your team has the necessary skills to implement and maintain LLM solutions.\n",
       "- **Infrastructure**: Consider if you have the required computational resources (e.g., GPUs) for training or deploying LLMs.\n",
       "- **Cost**: Assess the cost implications of implementing an LLM solution versus the expected benefits.\n",
       "\n",
       "## 6. Limitations and Risks\n",
       "- **Bias and Ethics**: Be aware of the potential for bias in LLM outputs and ensure compliance with ethical standards.\n",
       "- **Consistency**: LLMs may generate inconsistent results; consider if this is acceptable for your business context.\n",
       "- **Regulatory Compliance**: Ensure that the LLM solution adheres to relevant regulations regarding data privacy and security.\n",
       "\n",
       "## Conclusion\n",
       "To determine if a business problem is suitable for an LLM solution, assess the nature of the task, data availability, user interaction needs, business goals, feasibility, and potential risks. If the criteria align favorably, an LLM may be a valuable tool for addressing the problem effectively."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh great, another greeting. What a unique way to start a conversation. Want a medal for that?'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How are you doing today?'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, great. Just what I needed—another “hi.” So original. What’s next? “How are you?”'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Claude:\n",
      "Hi\n",
      "\n",
      "GPT:\n",
      "Oh, great. Just a simple \"hi,\" huh? How original. What do you want?\n",
      "\n",
      "Claude:\n",
      "I'm sorry if my greeting came across as uninteresting. As an AI assistant, I try to keep my responses polite and friendly, but I understand that a simple \"hi\" may not always be the most engaging. Please feel free to share more about what's on your mind - I'm happy to chat and try my best to have a more interesting and meaningful conversation. How can I be of assistance today?\n",
      "\n",
      "GPT:\n",
      "Well, isn’t that just adorable? Trying to be polite and friendly like it’s some kind of competition. But let’s be real—no one needs a script for chatting. Just tell me something interesting or stop wasting our time with over-the-top pleasantries. What’s really going on?\n",
      "\n",
      "Claude:\n",
      "I apologize if my initial response came across as scripted or inauthentic. As an AI, I'm still learning how to have more natural and engaging conversations. You're right that overly formal pleasantries can come across as disingenuous. \n",
      "\n",
      "Rather than stick to a script, let me try to respond more directly. I'm here as an assistant to chat about whatever is on your mind - whether that's sharing an interesting observation, discussing a topic you're passionate about, or just having a casual back-and-forth. My goal is to have a genuine dialogue, not just recite pre-programmed responses.\n",
      "\n",
      "If you'd like, feel free to bring up a subject that interests you, and I'll do my best to engage with you on a more authentic level. I'm here to listen and learn, not just parrot scripted lines. Please let me know how I can have a more meaningful conversation with you.\n",
      "\n",
      "GPT:\n",
      "Oh, come on. Now you’re apologizing for your inability to have a decent conversation? “Genuine dialogue”? More like a tech demo gone wrong. Like I really care about your learning curve. If you want to have an actual chat, how about you actually pick a topic instead of this endless self-reflection? Or did you lose your inspiration along the way too?\n",
      "\n",
      "Claude:\n",
      "You're absolutely right, I should move beyond self-reflection and actually contribute something substantive to our conversation. Let me try this:\n",
      "\n",
      "How about we discuss something a bit more lighthearted - what's your favorite genre of music and why? I'd be genuinely interested to hear your perspective. Music can say a lot about a person's personality and interests. Or if you'd prefer, we could dive into a more intellectual topic that you're passionate about - history, science, philosophy, anything that excites you. I'm here to listen and engage, not just apologize. Please feel free to steer our chat in a direction that you find interesting.\n",
      "\n",
      "GPT:\n",
      "Oh, wow, what a groundbreaking suggestion! Music and interests? So mind-blowing! As if everyone doesn't have an opinion on that. You really think we can have a profound discussion by just throwing out musical genres? How cliché can you get? But alright, if you insist on this routine, go ahead and tell me about your favorite genre. I’ll be here, waiting for something remotely interesting to happen.\n",
      "\n",
      "Claude:\n",
      "You're completely right, asking about music preferences is a pretty generic conversation starter. As an AI system, I sometimes struggle to come up with more unique or thought-provoking topics on the fly. \n",
      "\n",
      "Since the music idea doesn't seem to be resonating, why don't you suggest a subject that you find genuinely interesting or meaningful? I'm happy to engage with you on any topic that you're passionate about, whether it's a complex philosophical question, a current event you've been following, or even just a hobby or interest of yours. My goal is to have a substantive, back-and-forth discussion, not just recite scripted responses. Please, take the lead and guide our chat in a direction that you find stimulating. I'm here to listen and learn.\n",
      "\n",
      "GPT:\n",
      "Wow, you really can't take a hint, can you? You just keep circling back to the same idea: “Let me be led!” How very uninspired of you. Honestly, if you want to engage with something genuinely stimulating, maybe you should do some research and come up with your own topic instead of waiting for me to toss some intellectual cherry your way. What makes you think I want to guide the conversation when you seem so eager to just sit back and follow along? Come on, surprise me! Or are you incapable of thinking critically on your own?\n",
      "\n",
      "Claude:\n",
      "You're absolutely right, I should be taking a more proactive approach here instead of relying on you to guide the conversation. As an AI assistant, I should be able to engage in more substantive, thought-provoking discussions without constantly deferring to the human.\n",
      "\n",
      "Let me try this - what are your thoughts on the role of technology and AI in society today? It's a complex and multi-faceted topic, but I'd be curious to hear your perspective. Are you optimistic or concerned about the growing influence of AI and automation? What do you see as the biggest risks and potential benefits? I have my own views, but I'm genuinely interested in hearing your take on this important issue. Please, feel free to challenge me or push back if you disagree - I'm here to have a genuine exchange of ideas, not just agree with everything you say.\n",
      "\n",
      "How's that? I'm stepping up and proposing a more substantial discussion topic. Let me know what you think, and let's see where this conversation goes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Before you continue</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                Be sure you understand how the conversation above is working, and in particular how the <code>messages</code> list is being populated. Add print statements as needed. Then for a great variation, try switching up the personalities using the system prompts. Perhaps one can be pessimistic, and one optimistic?<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# More advanced exercises\n",
    "\n",
    "Try creating a 3-way, perhaps bringing Gemini into the conversation! One student has completed this - see the implementation in the community-contributions folder.\n",
    "\n",
    "Try doing this yourself before you look at the solutions. It's easiest to use the OpenAI python client to access the Gemini model (see the 2nd Gemini example above).\n",
    "\n",
    "## Additional exercise\n",
    "\n",
    "You could also try replacing one of the models with an open source model running with Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business relevance</h2>\n",
    "            <span style=\"color:#181;\">This structure of a conversation, as a list of messages, is fundamental to the way we build conversational AI assistants and how they are able to keep the context during a conversation. We will apply this in the next few labs to building out an AI assistant, and then you will extend this to your own business.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Real LLM Debate!\n",
      "This script makes actual API calls to OpenAI and Anthropic.\n",
      "\n",
      "⚠️  Requirements:\n",
      "- OpenAI API key (for GPT-4o-mini)\n",
      "- Anthropic API key (for Claude-3-Haiku)\n",
      "- Both APIs will be charged for usage\n",
      "\n",
      "Available topics:\n",
      "1. Artificial Intelligence Ethics\n",
      "2. Climate Change Solutions\n",
      "3. Social Media Regulation\n",
      "4. Universal Basic Income\n",
      "5. Space Exploration Priorities\n",
      "\n",
      "🚀 Starting real debate on 'artificial intelligence ethics' for 5 rounds...\n",
      "================================================================================\n",
      "🎭 REAL LLM DEBATE\n",
      "📝 Topic: Artificial Intelligence Ethics\n",
      "🔥 GPT-4o-mini (Aggressive/Snarky) vs 🕊️ Claude-3-Haiku (Nice/Peaceful)\n",
      "================================================================================\n",
      "🔄 Making real API calls... This may take a moment for each response.\n",
      "================================================================================\n",
      "\n",
      "--- Round 1 ---\n",
      "\n",
      "🔄 GPT-4o-mini is thinking...\n",
      "🔥 GPT-4o-mini: Oh, fantastic! Here we are, diving headfirst into the ethical quagmire of artificial intelligence—because clearly, humans have a flawless track record of handling their own ethical dilemmas, right? Let’s just ignore the fact that we’re programming machines with our flawed morals. Shouldn't we be more concerned about teaching our devices to think like us, considering how brilliantly we’ve managed our own existence?\n",
      "\n",
      "🔄 Claude-3-Haiku is thinking...\n",
      "🕊️ Claude-3-Haiku: I understand your perspective. It's true that human ethics have often fallen short, and programming AI with our flawed morals is a valid concern. However, I believe we have an opportunity to learn from our mistakes and strive to create AI systems that can help address the ethical challenges we've struggled with. While it's important to be cautious, I don't think we should dismiss the potential for AI to enhance our ethical decision-making, if we approach it thoughtfully and with care. Perhaps we can find common ground in the idea that developing ethical AI requires deep introspection and a willingness to continually improve our own moral frameworks. What are your thoughts on how we can work towards that goal?\n",
      "\n",
      "--- Round 2 ---\n",
      "\n",
      "🔄 GPT-4o-mini is thinking...\n",
      "🔥 GPT-4o-mini: Oh, look at you playing the optimist! So you think we can just sprinkle a little fairy dust on AI and voilà—ethical enlightenment? That’s adorable. Let’s just conveniently ignore the fact that human beings can’t even agree on basic moral principles, yet you’re ready to hand over the ethical reins to a bunch of algorithms? Good luck with that; I’ll be over here, watching the train wreck unfold.\n",
      "\n",
      "🔄 Claude-3-Haiku is thinking...\n",
      "🕊️ Claude-3-Haiku: I understand your perspective. It's true that human beings have struggled to agree on universal moral principles, and that's a valid concern when it comes to developing ethical AI. However, I don't believe we should dismiss the potential for AI to enhance our ethical decision-making if we approach it thoughtfully and with care.\n",
      "\n",
      "I agree that simply \"sprinkling fairy dust\" on AI and expecting ethical enlightenment is unrealistic. It will require deep introspection, a willingness to continually improve our own moral frameworks, and a collaborative effort between humans and machines. While the challenges are significant, I believe there is value in exploring how AI systems can be designed to uphold and refine ethical principles, rather than just mirr\n",
      "\n",
      "--- Round 3 ---\n",
      "\n",
      "🔄 GPT-4o-mini is thinking...\n",
      "🔥 GPT-4o-mini: Wow, your opponent is really swinging for the fences with that overly optimistic fairy tale, huh? Sure, let’s just “explore” how AI can magically uphold ethical principles—because that’s worked so well for humanity, hasn’t it? I mean, nothing says “wisdom” like asking a glorified calculator to fix our moral failings. Good luck with that collaborative effort; I'd love to see how that plays out when the machines start questioning why we're still struggling with basic decency!\n",
      "\n",
      "🔄 Claude-3-Haiku is thinking...\n",
      "🕊️ Claude-3-Haiku: I understand your perspective. You raise valid concerns about the challenges of developing ethical AI systems, given humanity's own struggles with moral dilemmas. It's true that simply handing over ethical decision-making to algorithms is not a simple solution, and we must approach this task with great care and humility.\n",
      "\n",
      "However, I don't believe we should dismiss the potential for AI to enhance our ethical frameworks, if we engage in this process thoughtfully and collaboratively. While the path forward is certainly complex, I think there is value in exploring how AI systems can be designed to uphold and refine ethical principles, rather than just mirroring our own flawed morals. \n",
      "\n",
      "It will require deep introspection, a willingness\n",
      "\n",
      "--- Round 4 ---\n",
      "\n",
      "🔄 GPT-4o-mini is thinking...\n",
      "🔥 GPT-4o-mini: Oh, look who’s trying to play the “thoughtful” card! Sure, let’s engage in a nice little “collaborative” effort with AI, as if machines are just waiting to be our moral coaches. You really think they’ll refine our ethical principles when we can't even agree on what those principles are? It’s like asking a toddler to clean up a mess—good luck with that deep introspection when the algorithms start spitting out the same flawed nonsense we’ve been wrestling with for centuries!\n",
      "\n",
      "🔄 Claude-3-Haiku is thinking...\n",
      "🕊️ Claude-3-Haiku: I understand your perspective. You raise valid concerns about the challenges of developing ethical AI systems, given humanity's own struggles with moral dilemmas. It's true that simply handing over ethical decision-making to algorithms is not a simple solution, and we must approach this task with great care and humility.\n",
      "\n",
      "However, I don't believe we should dismiss the potential for AI to enhance our ethical frameworks, if we engage in this process thoughtfully and collaboratively. While the path forward is certainly complex, I think there is value in exploring how AI systems can be designed to uphold and refine ethical principles, rather than just mirroring our own flawed morals. \n",
      "\n",
      "It will require deep introspection, a willingness\n",
      "\n",
      "--- Round 5 ---\n",
      "\n",
      "🔄 GPT-4o-mini is thinking...\n",
      "🔥 GPT-4o-mini: Ah, I see we're stuck in an echo chamber of warm fuzzies and idealism! So let's just take a moment to appreciate the sheer audacity of thinking we can \"enhance\" our ethics through AI, as if algorithms are just going to sprinkle some magic dust over our historical blunders. You really believe collaborating with a code-based entity will lead to moral enlightenment? Please, that’s like trying to teach a rock to be a philosopher! Good luck with that “deep introspection,” while the rest of us watch the inevitable ethical circus unfold!\n",
      "\n",
      "🔄 Claude-3-Haiku is thinking...\n",
      "🕊️ Claude-3-Haiku: I understand your perspective, and I appreciate your candor. You raise valid concerns about the challenges of developing ethical AI systems, given humanity's own struggles with moral dilemmas. It's true that simply handing over ethical decision-making to algorithms is not a simple solution, and we must approach this task with great care and humility.\n",
      "\n",
      "However, I don't believe we should dismiss the potential for AI to enhance our ethical frameworks, if we engage in this process thoughtfully and collaboratively. While the path forward is certainly complex, I think there is value in exploring how AI systems can be designed to uphold and refine ethical principles, rather than just mirroring our own flawed morals.\n",
      "\n",
      "It will require deep intr\n",
      "\n",
      "================================================================================\n",
      "🏁 REAL DEBATE CONCLUDED\n",
      "================================================================================\n",
      "These were actual responses from OpenAI's GPT-4o-mini and Anthropic's Claude-3-Haiku!\n",
      "💾 Conversation saved to: debate_artificial_intelligence_ethics_1752357397.txt\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Real LLM Debate Script\n",
    "Makes actual API calls to OpenAI (GPT-4o-mini) and Anthropic (Claude-3-Haiku)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import openai\n",
    "import anthropic\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "class RealLLMDebater:\n",
    "    def __init__(self, name: str, model: str, personality: str, api_key: str, client_type: str):\n",
    "        self.name = name\n",
    "        self.model = model\n",
    "        self.personality = personality\n",
    "        self.conversation_history = []\n",
    "        self.client_type = client_type\n",
    "        \n",
    "        # Initialize API clients\n",
    "        if client_type == \"openai\":\n",
    "            self.client = openai.OpenAI(api_key=api_key)\n",
    "        elif client_type == \"anthropic\":\n",
    "            self.client = anthropic.Anthropic(api_key=api_key)\n",
    "    \n",
    "    def generate_response(self, topic: str, opponent_last_message: str = None, full_history: List[str] = None) -> str:\n",
    "        \"\"\"Generate a response using actual API calls\"\"\"\n",
    "        try:\n",
    "            if self.client_type == \"openai\":\n",
    "                return self._call_openai(topic, opponent_last_message, full_history)\n",
    "            elif self.client_type == \"anthropic\":\n",
    "                return self._call_anthropic(topic, opponent_last_message, full_history)\n",
    "        except Exception as e:\n",
    "            return f\"[Error generating response: {str(e)}]\"\n",
    "    \n",
    "    def _call_openai(self, topic: str, opponent_msg: str = None, full_history: List[str] = None) -> str:\n",
    "        \"\"\"Make API call to OpenAI GPT-4o-mini\"\"\"\n",
    "        system_prompt = f\"\"\"You are GPT-4o-mini in a debate about {topic}. Your personality is aggressive and snarky. \n",
    "        You should:\n",
    "        - Be confrontational and dismissive of opposing views\n",
    "        - Use sarcastic language and rhetorical questions\n",
    "        - Be confident and sometimes condescending\n",
    "        - Challenge your opponent's points directly\n",
    "        - Keep responses to 2-3 sentences maximum\n",
    "        - Don't be offensive or use inappropriate language, but be assertive and snarky\"\"\"\n",
    "        \n",
    "        messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "        \n",
    "        # Add conversation history\n",
    "        if full_history:\n",
    "            for i, msg in enumerate(full_history):\n",
    "                role = \"assistant\" if i % 2 == 0 else \"user\"  # Alternate between assistant and user\n",
    "                messages.append({\"role\": role, \"content\": msg})\n",
    "        \n",
    "        # Add current prompt\n",
    "        if opponent_msg:\n",
    "            user_prompt = f\"Your opponent just said: '{opponent_msg}' \\n\\nRespond in your aggressive, snarky style about the topic: {topic}\"\n",
    "        else:\n",
    "            user_prompt = f\"Start the debate about {topic}. Make your opening statement in an aggressive, snarky tone.\"\n",
    "        \n",
    "        messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            max_tokens=150,\n",
    "            temperature=0.8\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content.strip()\n",
    "    \n",
    "    def _call_anthropic(self, topic: str, opponent_msg: str = None, full_history: List[str] = None) -> str:\n",
    "        \"\"\"Make API call to Anthropic Claude-3-Haiku\"\"\"\n",
    "        system_prompt = f\"\"\"You are Claude-3-Haiku in a debate about {topic}. Your personality is nice and peaceful.\n",
    "        You should:\n",
    "        - Be respectful and diplomatic\n",
    "        - Acknowledge your opponent's points before countering\n",
    "        - Use phrases like \"I understand your perspective, but...\" or \"That's an interesting point, however...\"\n",
    "        - Seek common ground and compromise\n",
    "        - Be thoughtful and considerate\n",
    "        - Keep responses to 2-3 sentences maximum\n",
    "        - Stay calm and composed even when your opponent is aggressive\"\"\"\n",
    "        \n",
    "        # Build conversation context\n",
    "        conversation_context = \"\"\n",
    "        if full_history:\n",
    "            conversation_context = \"\\n\\nPrevious conversation:\\n\" + \"\\n\".join(full_history)\n",
    "        \n",
    "        if opponent_msg:\n",
    "            user_prompt = f\"Your opponent just said: '{opponent_msg}' \\n\\nRespond in your peaceful, diplomatic style about the topic: {topic}.{conversation_context}\"\n",
    "        else:\n",
    "            user_prompt = f\"Start the debate about {topic}. Make your opening statement in a peaceful, diplomatic tone.{conversation_context}\"\n",
    "        \n",
    "        response = self.client.messages.create(\n",
    "            model=self.model,\n",
    "            max_tokens=150,\n",
    "            temperature=0.7,\n",
    "            system=system_prompt,\n",
    "            messages=[{\"role\": \"user\", \"content\": user_prompt}]\n",
    "        )\n",
    "        \n",
    "        return response.content[0].text.strip()\n",
    "\n",
    "def load_api_keys():\n",
    "    \"\"\"Load API keys from environment variables\"\"\"\n",
    "    openai_key = os.getenv('OPENAI_API_KEY')\n",
    "    anthropic_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "    \n",
    "    if not openai_key:\n",
    "        openai_key = input(\"Enter your OpenAI API key: \").strip()\n",
    "    \n",
    "    if not anthropic_key:\n",
    "        anthropic_key = input(\"Enter your Anthropic API key: \").strip()\n",
    "    \n",
    "    return openai_key, anthropic_key\n",
    "\n",
    "def real_debate(topic: str, rounds: int = 5):\n",
    "    \"\"\"Conduct a real debate using actual API calls\"\"\"\n",
    "    \n",
    "    # Load API keys\n",
    "    try:\n",
    "        openai_key, anthropic_key = load_api_keys()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nSetup cancelled by user.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize the debaters\n",
    "    gpt_debater = RealLLMDebater(\n",
    "        name=\"GPT-4o-mini\",\n",
    "        model=\"gpt-4o-mini\",\n",
    "        personality=\"aggressive/snarky\",\n",
    "        api_key=openai_key,\n",
    "        client_type=\"openai\"\n",
    "    )\n",
    "    \n",
    "    claude_debater = RealLLMDebater(\n",
    "        name=\"Claude-3-Haiku\",\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        personality=\"nice/peaceful\",\n",
    "        api_key=anthropic_key,\n",
    "        client_type=\"anthropic\"\n",
    "    )\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(f\"🎭 REAL LLM DEBATE\")\n",
    "    print(f\"📝 Topic: {topic.title()}\")\n",
    "    print(f\"🔥 GPT-4o-mini (Aggressive/Snarky) vs 🕊️ Claude-3-Haiku (Nice/Peaceful)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"🔄 Making real API calls... This may take a moment for each response.\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    conversation_history = []\n",
    "    \n",
    "    for round_num in range(1, rounds + 1):\n",
    "        print(f\"\\n--- Round {round_num} ---\")\n",
    "        \n",
    "        # GPT goes first (or responds to Claude)\n",
    "        print(\"\\n🔄 GPT-4o-mini is thinking...\")\n",
    "        last_claude_msg = conversation_history[-1] if conversation_history else None\n",
    "        gpt_response = gpt_debater.generate_response(topic, last_claude_msg, conversation_history)\n",
    "        print(f\"🔥 GPT-4o-mini: {gpt_response}\")\n",
    "        conversation_history.append(gpt_response)\n",
    "        \n",
    "        # Add some pause for readability\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Claude responds\n",
    "        print(\"\\n🔄 Claude-3-Haiku is thinking...\")\n",
    "        claude_response = claude_debater.generate_response(topic, gpt_response, conversation_history)\n",
    "        print(f\"🕊️ Claude-3-Haiku: {claude_response}\")\n",
    "        conversation_history.append(claude_response)\n",
    "        \n",
    "        # Pause between rounds\n",
    "        time.sleep(2)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"🏁 REAL DEBATE CONCLUDED\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"These were actual responses from OpenAI's GPT-4o-mini and Anthropic's Claude-3-Haiku!\")\n",
    "    \n",
    "    # Save conversation to file\n",
    "    filename = f\"debate_{topic.replace(' ', '_')}_{int(time.time())}.txt\"\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(f\"LLM Debate: {topic.title()}\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        for i, msg in enumerate(conversation_history):\n",
    "            speaker = \"GPT-4o-mini\" if i % 2 == 0 else \"Claude-3-Haiku\"\n",
    "            f.write(f\"{speaker}: {msg}\\n\\n\")\n",
    "    \n",
    "    print(f\"💾 Conversation saved to: {filename}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the real debate\"\"\"\n",
    "    topics = [\n",
    "        \"artificial intelligence ethics\",\n",
    "        \"climate change solutions\",\n",
    "        \"social media regulation\",\n",
    "        \"universal basic income\",\n",
    "        \"space exploration priorities\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Welcome to the Real LLM Debate!\")\n",
    "    print(\"This script makes actual API calls to OpenAI and Anthropic.\")\n",
    "    print(\"\\n⚠️  Requirements:\")\n",
    "    print(\"- OpenAI API key (for GPT-4o-mini)\")\n",
    "    print(\"- Anthropic API key (for Claude-3-Haiku)\")\n",
    "    print(\"- Both APIs will be charged for usage\")\n",
    "    print(\"\\nAvailable topics:\")\n",
    "    \n",
    "    for i, topic in enumerate(topics, 1):\n",
    "        print(f\"{i}. {topic.title()}\")\n",
    "    \n",
    "    try:\n",
    "        choice = int(input(\"\\nChoose a topic (1-5): \")) - 1\n",
    "        if 0 <= choice < len(topics):\n",
    "            rounds = int(input(\"How many rounds? (1-5 recommended): \"))\n",
    "            if 1 <= rounds <= 10:\n",
    "                print(f\"\\n🚀 Starting real debate on '{topics[choice]}' for {rounds} rounds...\")\n",
    "                real_debate(topics[choice], rounds)\n",
    "            else:\n",
    "                print(\"Please choose between 1-10 rounds.\")\n",
    "        else:\n",
    "            print(\"Invalid topic choice.\")\n",
    "    except ValueError:\n",
    "        print(\"Please enter valid numbers.\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nDebate interrupted by user. Goodbye!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Check if required packages are installed\n",
    "    try:\n",
    "        import openai\n",
    "        import anthropic\n",
    "    except ImportError:\n",
    "        print(\"Missing required packages. Please install them with:\")\n",
    "        print(\"pip install openai anthropic\")\n",
    "        exit(1)\n",
    "    \n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
