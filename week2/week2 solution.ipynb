{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddfa9ae6-69fe-444a-b994-8c4c5970a7ec",
   "metadata": {},
   "source": [
    "# Project - Airline AI Assistant\n",
    "\n",
    "We'll now bring together what we've learned to make an AI Customer Support assistant for an Airline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b50bbe2-c0b1-49c3-9a5c-1ba7efa2bcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "from faster_whisper import WhisperModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "747e8786-9da8-4342-b6c9-f5f69c2e22ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f60c22e0a6a4ca1984fbed6702a0688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocabulary.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3cea22170d452cbd9452726b36b6c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848a46deacb942368804700c89dac8df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.bin:   0%|          | 0.00/75.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc941aaa80d466eb6c33010ddc927cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialization\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "MODEL = \"gpt-4o-mini\"\n",
    "w_model = WhisperModel(\"tiny\",compute_type=\"int8\")\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a521d84-d07c-49ab-a0df-d6451499ed97",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant for an Airline called FlightAI. \"\n",
    "system_message += \"Give short, courteous answers, no more than 1 sentence. \"\n",
    "system_message += \"Always be accurate. If you don't know the answer, say so.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bedabf-a0a7-4985-ad8e-07ed6a55a3a4",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "Tools are an incredibly powerful feature provided by the frontier LLMs.\n",
    "\n",
    "With tools, you can write a function, and have the LLM call that function as part of its response.\n",
    "\n",
    "Sounds almost spooky.. we're giving it the power to run code on our machine?\n",
    "\n",
    "Well, kinda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0696acb1-0b05-4dc2-80d5-771be04f1fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by making a useful function\n",
    "\n",
    "ticket_prices = {\"london\": \"$799\", \"paris\": \"$899\", \"tokyo\": \"$1400\", \"berlin\": \"$499\"}\n",
    "\n",
    "def get_ticket_price(destination_city):\n",
    "    print(f\"Tool get_ticket_price called for {destination_city}\")\n",
    "    city = destination_city.lower()\n",
    "    return ticket_prices.get(city, \"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4afceded-7178-4c05-8fa6-9f2085e6a344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's a particular dictionary structure that's required to describe our function:\n",
    "\n",
    "price_function = {\n",
    "    \"name\": \"get_ticket_price\",\n",
    "    \"description\": \"Get the price of a return ticket to the destination city. Call this whenever you need to know the ticket price, for example when a customer asks 'How much is a ticket to this city'\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"destination_city\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The city that the customer wants to travel to\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"destination_city\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bdca8679-935f-4e7f-97e6-e71a4d4f228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And this is included in a list of tools:\n",
    "\n",
    "tools = [{\"type\": \"function\", \"function\": price_function}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d3554f-b4e3-4ce7-af6f-68faa6dd2340",
   "metadata": {},
   "source": [
    "## Getting OpenAI to use our Tool\n",
    "\n",
    "There's some fiddly stuff to allow OpenAI \"to call our tool\"\n",
    "\n",
    "What we actually do is give the LLM the opportunity to inform us that it wants us to run the tool.\n",
    "\n",
    "Here's how the new chat function looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b0992986-ea09-4912-a076-8e5603ee631f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to write that function handle_tool_call:\n",
    "\n",
    "def handle_tool_call(message):\n",
    "    tool_call = message.tool_calls[0]\n",
    "    arguments = json.loads(tool_call.function.arguments)\n",
    "    city = arguments.get('destination_city')\n",
    "    price = get_ticket_price(city)\n",
    "    response = {\n",
    "        \"role\": \"tool\",\n",
    "        \"content\": json.dumps({\"destination_city\": city,\"price\": price}),\n",
    "        \"tool_call_id\": tool_call.id\n",
    "    }\n",
    "    return response, city"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473e5b39-da8f-4db1-83ae-dbaca2e9531e",
   "metadata": {},
   "source": [
    "# Let's go multi-modal!!\n",
    "\n",
    "We can use DALL-E-3, the image generation model behind GPT-4o, to make us some images\n",
    "\n",
    "Let's put this in a function called artist.\n",
    "\n",
    "### Price alert: each time I generate an image it costs about 4 cents - don't go crazy with images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2c27c4ba-8ed5-492f-add1-02ce9c81d34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some imports for handling images\n",
    "\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "773a9f11-557e-43c9-ad50-56cbec3a0f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def artist(city):\n",
    "    image_response = openai.images.generate(\n",
    "            model=\"dall-e-3\",\n",
    "            prompt=f\"An image representing a vacation in {city}, showing tourist spots and everything unique about {city}, in a vibrant pop-art style\",\n",
    "            size=\"1024x1024\",\n",
    "            n=1,\n",
    "            response_format=\"b64_json\",\n",
    "        )\n",
    "    image_base64 = image_response.data[0].b64_json\n",
    "    image_data = base64.b64decode(image_base64)\n",
    "    return Image.open(BytesIO(image_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91d3f8f-e505-4e3c-a87c-9e42ed823db6",
   "metadata": {},
   "source": [
    "# For Mac users - and possibly many PC users too\n",
    "\n",
    "This version should work fine for you. It might work for Windows users too, but you might get a Permissions error writing to a temp file. If so, see the next section!\n",
    "\n",
    "As always, if you have problems, please contact me! (You could also comment out the audio talker() in the later code if you're less interested in audio generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ffbfe93b-5e86-4e68-ba71-b301cd5230db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "\n",
    "def talker(message):\n",
    "    response = openai.audio.speech.create(\n",
    "      model=\"tts-1\",\n",
    "      voice=\"onyx\",    # Also, try replacing onyx with alloy\n",
    "      input=message\n",
    "    )\n",
    "    \n",
    "    audio_stream = BytesIO(response.content)\n",
    "    audio = AudioSegment.from_file(audio_stream, format=\"mp3\")\n",
    "    play(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d48876d-c4fa-46a8-a04f-f9fadf61fb0d",
   "metadata": {},
   "source": [
    "# Our Agent Framework\n",
    "\n",
    "The term 'Agentic AI' and Agentization is an umbrella term that refers to a number of techniques, such as:\n",
    "\n",
    "1. Breaking a complex problem into smaller steps, with multiple LLMs carrying out specialized tasks\n",
    "2. The ability for LLMs to use Tools to give them additional capabilities\n",
    "3. The 'Agent Environment' which allows Agents to collaborate\n",
    "4. An LLM can act as the Planner, dividing bigger tasks into smaller ones for the specialists\n",
    "5. The concept of an Agent having autonomy / agency, beyond just responding to a prompt - such as Memory\n",
    "\n",
    "We're showing 1 and 2 here, and to a lesser extent 3 and 5. In week 8 we will do the lot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ba820c95-02f5-499e-8f3c-8727ee0a6c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history\n",
    "    response = openai.chat.completions.create(model=MODEL, messages=messages, tools=tools)\n",
    "    image = None\n",
    "    \n",
    "    if response.choices[0].finish_reason==\"tool_calls\":\n",
    "        message = response.choices[0].message\n",
    "        response, city = handle_tool_call(message)\n",
    "        messages.append(message)\n",
    "        messages.append(response)\n",
    "        image = artist(city)\n",
    "        response = openai.chat.completions.create(model=MODEL, messages=messages)\n",
    "        \n",
    "    reply = response.choices[0].message.content\n",
    "    history += [{\"role\":\"assistant\", \"content\":reply}]\n",
    "\n",
    "    # Comment out or delete the next line if you'd rather skip Audio for now..\n",
    "    talker(reply)\n",
    "    \n",
    "    return history, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38d0d27-33bf-4992-a2e5-5dbed973cde7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7899\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7899/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs received: {'text': '', 'files': ['/private/var/folders/wd/vhv7g6pj5dl50yr9xq6wtbgh0000gn/T/gradio/d2b9ca931e97223a6f210fdede4ceb605b7f1683600d284a0004ae965c01f840/audio.wav']}\n",
      "Audio input received: ['/private/var/folders/wd/vhv7g6pj5dl50yr9xq6wtbgh0000gn/T/gradio/d2b9ca931e97223a6f210fdede4ceb605b7f1683600d284a0004ae965c01f840/audio.wav']\n",
      "Tool get_ticket_price called for London\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input #0, wav, from '/var/folders/wd/vhv7g6pj5dl50yr9xq6wtbgh0000gn/T/tmpnfy1r80e.wav':\n",
      "  Duration: 00:00:02.28, bitrate: 384 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 24000 Hz, 1 channels, s16, 384 kb/s\n",
      "   2.18 M-A: -0.000 fd=   0 aq=    0KB vq=    0KB sq=    0B \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# More involved Gradio code as we're not using the preset Chat interface!\n",
    "# Passing in inbrowser=True in the last line will cause a Gradio window to pop up immediately.\n",
    "# Load transcription model\n",
    "import mimetypes\n",
    "whisp_model = WhisperModel(\"tiny\", compute_type=\"int8\")\n",
    "\n",
    "# Guess MIME types\n",
    "def guess_mime(path):\n",
    "    mime, _ = mimetypes.guess_type(path)\n",
    "    return mime or \"application/octet-stream\"\n",
    "\n",
    "def transcribe(audio_path):\n",
    "    segments, _ = whisp_model.transcribe(audio_path, vad_filter=False)\n",
    "    segments = list(segments)\n",
    "    full_text = \" \".join([seg.text for seg in segments])\n",
    "    return full_text.strip()\n",
    "\n",
    "with gr.Blocks() as ui:\n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(height=500, type=\"messages\")\n",
    "        image_output = gr.Image(height=500)\n",
    "        state = gr.State([])\n",
    "    with gr.Row():\n",
    "        entry = gr.MultimodalTextbox(\n",
    "            interactive=True,\n",
    "            file_count=\"single\",\n",
    "            placeholder=\"Chat by typing or talking with our AI Assistant:\",\n",
    "            show_label=True,\n",
    "            sources=[\"microphone\", \"upload\"],\n",
    "            )\n",
    "    with gr.Row():\n",
    "        clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def do_entry(inputs, history):\n",
    "        text_input = \"\"\n",
    "        audio_path = None\n",
    "\n",
    "        text_input = inputs.get(\"text\", \"\")\n",
    "        file_paths = inputs.get(\"files\", [])\n",
    "    \n",
    "        # Separate by type\n",
    "        audio_path = [f for f in file_paths if guess_mime(f).startswith(\"audio/\")]\n",
    "        image_files = [f for f in file_paths if guess_mime(f).startswith(\"image/\")]\n",
    "\n",
    "    # Transcribe audio\n",
    "        audio_text = transcribe(audio_path[0])\n",
    "        merged_input = f\"{text_input} {audio_text}\".strip()\n",
    "        message = {\"role\": \"user\", \"content\": merged_input}\n",
    "        if not merged_input:\n",
    "            return history\n",
    "        history.append(message)\n",
    "        return \"\", history\n",
    "\n",
    "    entry.submit(do_entry, inputs=[entry, chatbot], outputs=[entry, chatbot]).then(\n",
    "        chat, inputs=chatbot, outputs=[chatbot, image_output]\n",
    "    )\n",
    "    clear.click(lambda: None, inputs=None, outputs=chatbot, queue=False)\n",
    "\n",
    "ui.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226643d2-73e4-4252-935d-86b8019e278a",
   "metadata": {},
   "source": [
    "# Exercises and Business Applications\n",
    "\n",
    "Add in more tools - perhaps to simulate actually booking a flight. A student has done this and provided their example in the community contributions folder.\n",
    "\n",
    "Next: take this and apply it to your business. Make a multi-modal AI assistant with tools that could carry out an activity for your work. A customer support assistant? New employee onboarding assistant? So many possibilities! Also, see the week2 end of week Exercise in the separate Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e795560-1867-42db-a256-a23b844e6fbe",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../thankyou.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#090;\">I have a special request for you</h2>\n",
    "            <span style=\"color:#090;\">\n",
    "                My editor tells me that it makes a HUGE difference when students rate this course on Udemy - it's one of the main ways that Udemy decides whether to show it to others. If you're able to take a minute to rate this, I'd be so very grateful! And regardless - always please reach out to me at ed@edwarddonner.com if I can help at any point.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d7cf8e",
   "metadata": {},
   "source": [
    "# Additional End of week Exercise - week 2\n",
    "\n",
    "Now use everything you've learned from Week 2 to build a full prototype for the technical question/answerer you built in Week 1 Exercise.\n",
    "\n",
    "This should include a Gradio UI, streaming, use of the system prompt to add expertise, and the ability to switch between models. Bonus points if you can demonstrate use of a tool!\n",
    "\n",
    "If you feel bold, see if you can add audio input so you can talk to it, and have it respond with audio. ChatGPT or Claude can help you, or email me if you have questions.\n",
    "\n",
    "I will publish a full solution here soon - unless someone beats me to it...\n",
    "\n",
    "There are so many commercial applications for this, from a language tutor, to a company onboarding solution, to a companion AI to a course (like this one!) I can't wait to see your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e071537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording... Speak now!\n",
      "You said: Jesus, what's your name?\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import scipy.io.wavfile\n",
    "\n",
    "def record_audio(duration=5, fs=16000):\n",
    "    print(\"Recording... Speak now!\")\n",
    "    audio = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype='int16')\n",
    "    sd.wait()\n",
    "    return audio, fs\n",
    "\n",
    "def speech_to_text():\n",
    "    audio, fs = record_audio()\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmpfile:\n",
    "        scipy.io.wavfile.write(tmpfile.name, fs, audio)\n",
    "        with open(tmpfile.name, \"rb\") as audio_file:\n",
    "            transcript = openai.audio.transcriptions.create(\n",
    "                model=\"whisper-1\",\n",
    "                file=audio_file\n",
    "            )\n",
    "    return transcript.text\n",
    "\n",
    "# Example usage:\n",
    "spoken_text = speech_to_text()\n",
    "\n",
    "#history, image = chat([{\"role\": \"user\", \"content\": spoken_text}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ea7bd0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wd/vhv7g6pj5dl50yr9xq6wtbgh0000gn/T/ipykernel_81667/1301059120.py:50: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbox = gr.Chatbot()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "from faster_whisper import WhisperModel\n",
    "import openai\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "model = WhisperModel(\"tiny\", compute_type=\"int8\")\n",
    "\n",
    "def transcribe_audio(audio_path):\n",
    "    if not audio_path:\n",
    "        return \"\"\n",
    "    segments, _ = model.transcribe(audio_path)\n",
    "    return \" \".join([s.text for s in segments]).strip()\n",
    "\n",
    "def handle_input(text, audio, history=[]):\n",
    "    audio_text = transcribe_audio(audio)\n",
    "    full_input = f\"{text or ''} {audio_text or ''}\".strip()\n",
    "\n",
    "    if not full_input:\n",
    "        return history\n",
    "\n",
    "    history.append((f\"ðŸ—£ï¸ {full_input}\", \"\"))\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "    for user_msg, bot_msg in history:\n",
    "        messages.append({\"role\": \"user\", \"content\": user_msg.replace(\"ðŸ—£ï¸ \", \"\")})\n",
    "        if bot_msg:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": bot_msg})\n",
    "\n",
    "    response = openai.chat.completions.create(\n",
    "        model=MODEL, messages=messages\n",
    "    )\n",
    "    reply = response.choices[0].message.content.strip()\n",
    "    history[-1] = (history[-1][0], f\"ðŸ¤– {reply}\")\n",
    "    return history\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"ðŸŽ¤ðŸ’¬ Chatbot with Text + Microphone Audio Input\")\n",
    "    state = gr.State([])\n",
    "\n",
    "    with gr.Row():\n",
    "        text_input = gr.Textbox(label=\"Type your message\")\n",
    "        audio_input = gr.Audio(sources=[\"microphone\", \"upload\"], type=\"filepath\", label=\"Or speak\")\n",
    "\n",
    "    submit = gr.Button(\"Submit\")\n",
    "    clear = gr.Button(\"Clear\")\n",
    "    chatbox = gr.Chatbot()\n",
    "\n",
    "    submit.click(\n",
    "        handle_input, \n",
    "        inputs=[text_input, audio_input, state], \n",
    "        outputs=[chatbox]\n",
    "    )\n",
    "\n",
    "    clear.click(lambda: ([], [], []), None, outputs=[text_input, audio_input, chatbox, state])\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "03f6b7ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'gradio' has no attribute 'Output'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m submit_button = gr.Button(\u001b[33m\"\u001b[39m\u001b[33mSubmit\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Display the output\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m output = \u001b[43mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mOutput\u001b[49m()\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess_audio\u001b[39m(audio):\n\u001b[32m     24\u001b[39m     \u001b[38;5;66;03m# You will need to process the audio here if you want\u001b[39;00m\n\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m# For demonstration, we will just output that audio was received.\u001b[39;00m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAudio received, processing is not shown in this example.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAttributeError\u001b[39m: module 'gradio' has no attribute 'Output'"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Function to process the input text (you can modify this to do anything you want)\n",
    "def process_input(text):\n",
    "    return f\"You said or typed: '{text}'\"\n",
    "\n",
    "# Create a Gradio interface with both text input and microphone capabilities\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## Multi-Model Text Box Example\")\n",
    "    \n",
    "    # Textbox for direct text input\n",
    "    text_box = gr.Textbox(label=\"Type or Speak\", placeholder=\"Type something here...\")\n",
    "\n",
    "    # Add a microphone button for speech input\n",
    "    microphone = gr.Audio(sources=\"microphone\", type=\"numpy\", label=\"Or use your microphone\")\n",
    "\n",
    "    # Button to process the input\n",
    "    submit_button = gr.Button(\"Submit\")\n",
    "\n",
    "    # Display the output\n",
    "    output = gr.Output()\n",
    "\n",
    "    def process_audio(audio):\n",
    "        # You will need to process the audio here if you want\n",
    "        # For demonstration, we will just output that audio was received.\n",
    "        return \"Audio received, processing is not shown in this example.\"\n",
    "\n",
    "    # Defining the actions for button clicks\n",
    "    submit_button.click(fn=process_input, inputs=text_box, outputs=output)\n",
    "    microphone.change(fn=process_audio, inputs=microphone, outputs=output)\n",
    "\n",
    "# Launch the Gradio interface\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
